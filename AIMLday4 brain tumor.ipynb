{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c689b67b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.16.1\n",
      "  Downloading tensorflow_intel-2.16.1-cp39-cp39-win_amd64.whl (376.9 MB)\n",
      "     -------------------------------------- 376.9/376.9 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (63.4.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting keras>=3.0.0\n",
      "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (21.3)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.64.1-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.7/133.7 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.5.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     -------------------------------------- 413.4/413.4 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.3.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "     ---------------------------------------- 26.4/26.4 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Collecting h5py>=3.10.0\n",
      "  Downloading h5py-3.11.0-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp39-cp39-win_amd64.whl (127 kB)\n",
      "     -------------------------------------- 127.7/127.7 kB 7.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.37.1)\n",
      "Collecting optree\n",
      "  Downloading optree-0.11.0-cp39-cp39-win_amd64.whl (240 kB)\n",
      "     -------------------------------------- 240.0/240.0 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "     -------------------------------------- 240.7/240.7 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.3.4)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ssaro\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.16.1->tensorflow) (3.0.9)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.5/87.5 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.5.5-py3-none-any.whl size=20090 sha256=104e60e1d39a148b81e73d531f466fd53b7c595f9bd22d3518cd889b25aacaea\n",
      "  Stored in directory: c:\\users\\ssaro\\appdata\\local\\pip\\cache\\wheels\\2d\\65\\07\\3404cdfbf1c7600209f7f6337d519e117442affbf2c6e79558\n",
      "Successfully built gast\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, pygments, protobuf, optree, numpy, mdurl, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, opt-einsum, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.5 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 pygments-2.18.0 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.2.2 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.2.2 requires pyqtwebengine<5.13, which is not installed.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "scipy 1.9.1 requires numpy<1.25.0,>=1.18.5, but you have numpy 1.26.4 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install --upgrade module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f38ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssaro\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#define image size and batch size\n",
    "IMG_SIZE=224\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e886b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#define data generator for train,validation and sets\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,validation_split=0.2)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\"C:\\Train-20240627T050055Z-001\\Train\",\n",
    "target_size=(IMG_SIZE,IMG_SIZE),\n",
    "batch_size=BATCH_SIZE,\n",
    "class_mode='binary',\n",
    "subset='training'\n",
    " )                                                 \n",
    "val_generator=train_datagen.flow_from_directory(\"C:\\Train-20240627T050055Z-001\\Train\",\n",
    "target_size=(IMG_SIZE,IMG_SIZE),\n",
    "batch_size=BATCH_SIZE,\n",
    "class_mode='binary',\n",
    "subset='validation'\n",
    ")                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091a5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "model=keras.Sequential([\n",
    "    layers.Conv2D(32,(3,3),activation='relu',input_shape=(IMG_SIZE,IMG_SIZE,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128,(3,3),activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128,activation='relu'),\n",
    "    layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5d5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba6ce1c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssaro\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.7060 - loss: 0.6196 - val_accuracy: 0.8600 - val_loss: 0.3133\n",
      "Epoch 2/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 801ms/step - accuracy: 0.9041 - loss: 0.2404 - val_accuracy: 0.9333 - val_loss: 0.1502\n",
      "Epoch 3/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 800ms/step - accuracy: 0.9595 - loss: 0.1191 - val_accuracy: 0.9850 - val_loss: 0.0575\n",
      "Epoch 4/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 882ms/step - accuracy: 0.9904 - loss: 0.0379 - val_accuracy: 0.9933 - val_loss: 0.0179\n",
      "Epoch 5/5\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 859ms/step - accuracy: 0.9895 - loss: 0.0321 - val_accuracy: 0.9983 - val_loss: 0.0093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cf3efdf4c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,validation_data=val_generator,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64d1575e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd1aa51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssaro\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "#load the saved model\n",
    "model=load_model(r\"C:\\Users\\ssaro\\Model.h5\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e605d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path=(r\"C:\\Train-20240627T050055Z-001\\Train\\no\\no25.jpg\")\n",
    "img=image.load_img(test_image_path,target_size=(224, 224))\n",
    "img_array=image.img_to_array(img)\n",
    "img_array=np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0871edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step\n",
      "[[4.948637e-06]]\n"
     ]
    }
   ],
   "source": [
    "#add batch dimension\n",
    "img_array/=255. #normalize the pixel value\n",
    "#make predictions\n",
    "prediction=model.predict(img_array)\n",
    "#print the prediction\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9918c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:No tumor(probability: 4.948637e-06\n"
     ]
    }
   ],
   "source": [
    "if prediction<0.5:\n",
    "    print(\"prediction:No tumor(probability:\",prediction[0][0])\n",
    "else:\n",
    "    print(\"prediction:tumor present(probability:\",prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d87606",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path=(r\"C:\\Train-20240627T050055Z-001\\Train\\yes\\y10.jpg\")\n",
    "img=image.load_img(test_image_path,target_size=(224, 224))\n",
    "img_array=image.img_to_array(img)\n",
    "img_array=np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e610cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "[[0.9999997]]\n"
     ]
    }
   ],
   "source": [
    "#add batch dimension\n",
    "img_array/=255. #normalize the pixel value\n",
    "#make predictions\n",
    "prediction=model.predict(img_array)\n",
    "#print the prediction\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1547bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:tumor present(probability: 0.9999997\n"
     ]
    }
   ],
   "source": [
    "if prediction<0.5:\n",
    "    print(\"prediction:No tumor(probability:\",prediction[0][0])\n",
    "else:\n",
    "    print(\"prediction:tumor present(probability:\",prediction[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdd4cc",
   "metadata": {},
   "source": [
    "notes:CNN algorithm\n",
    "\n",
    "1.importing libraries\n",
    "2.declear instraction and properties to split the data\n",
    "3.apply these properties into \n",
    "4.build the model with colv2D pooling and flatted layer\n",
    "5.compile model with an optimizer , loss function,accuracy\n",
    "6.train the model with training data\n",
    "7.to reuse save the model with extension .h5\n",
    "\n",
    "\n",
    "1.testing\n",
    "2.load the model\n",
    "3.reshape the image acconding to shape given in the trained model\n",
    "4.convert the image into array using imagetoarray() function\n",
    "5.rescale image\n",
    "6.predict image\n",
    "\n",
    "\n",
    "interpreting image\n",
    "select a file from everey class and see the prediction value\n",
    "example:no tumor-4.94 \n",
    "        yes tumor-0.99\n",
    "        \n",
    "based on your prediction write an if else case        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a185eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93db9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data set\n",
    "#sample databases\n",
    "data={\n",
    "    'BP':[120,130,140,150,160,170,180,190,200,210],\n",
    "    'Cholesterol':[200,220,240,260,280,300,320,340,360,380],\n",
    "    'HeartRisk':[0,0,0,0,1,1,1,1,1,1]#0:no risk,1:at risk\n",
    "}\n",
    "\n",
    "#create dataframe\n",
    "df=pd.DataFrame(data)\n",
    "#create dataframe\n",
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ed6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['BP','Cholesterol']]\n",
    "y=df['HeartRisk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8e25de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=3\n",
    "knn=KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ed4f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At risk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssaro\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ssaro\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "#example prediction\n",
    "new_data = np.array([[210, 250]]) #new patient with BP-145 and Cholesterol-250\n",
    "prediction = knn.predict(new_data)\n",
    "\n",
    "if prediction == 0:\n",
    "    print(\"No risk\")\n",
    "else:\n",
    "    print(\"At risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea89f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
